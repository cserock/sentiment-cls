{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bT5lgAn7Ak5x"
      },
      "source": [
        "# 커뮤니티 포스트 감정분석을 위한 KoBERT 기반 Multi Classification 모델"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4oDww_uztpTp"
      },
      "source": [
        "## 환경 설정"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "WMSbbjMHtpTq"
      },
      "outputs": [],
      "source": [
        "PROJECT_NAME = \"tbb-model-api\"\n",
        "MODEL_PATH = f'/Users/rock/Documents/tumblbug-codes/{PROJECT_NAME}/packages/model/sentiment_classification'\n",
        "DATASET_PATH = f'/Users/rock/Documents/tumblbug-codes/{PROJECT_NAME}/packages/model/sentiment_classification/data'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "stAjiC7UtpTq"
      },
      "outputs": [],
      "source": [
        "!uv pip install -r requirements.txt"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VPc4o1biyWl7"
      },
      "source": [
        "### 데이터 분포"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "onqJ3qj2ybVw"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "\n",
        "df = pd.read_csv(f'{DATASET_PATH}/ko_sentence_template_origin.csv')\n",
        "# ==========================\n",
        "# origin data 분포\n",
        "# --------------------------\n",
        "# template\n",
        "# positive    0.414991\n",
        "# neutral     0.386045\n",
        "# negative    0.198964\n",
        "# Name: proportion, dtype: float64\n",
        "print('==========================')\n",
        "print('origin data 분포')\n",
        "print('--------------------------')\n",
        "print(df['template'].value_counts(normalize=True))\n",
        "# df['template'].hist(figsize=(5,5))\n",
        "\n",
        "df['template'].hist(bins=20, alpha=0.5, edgecolor = 'black')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "efKLPDuDeEfs"
      },
      "source": [
        "### 데이터셋 전처리"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "y8Gcb9myeI2a"
      },
      "outputs": [],
      "source": [
        "#train & test 데이터로 나누기\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "df = pd.read_csv(f'{DATASET_PATH}/ko_sentence_template_origin.csv')\n",
        "# df.head(100)\n",
        "# df.shape\n",
        "\n",
        "# 특수문자 제거\n",
        "df['sentence'] = df['sentence'].str.replace(pat=r'[^\\w\\s]', repl=r'', regex=True)\n",
        "df['sentence'] = df['sentence'].str.replace(',', ' ')\n",
        "\n",
        "# 줄바꿈 제거\n",
        "df['sentence'] = df['sentence'].str.replace('\\s+', ' ')\n",
        "df['sentence'] = df['sentence'].str.replace('\\n', ' ')\n",
        "\n",
        "# 공백 포함 행 제거\n",
        "df = df.replace('', pd.NA)\n",
        "df = df.dropna()\n",
        "\n",
        "neutral_samples = df[df['template'] == 'neutral'].sample(n=100000, random_state=42)\n",
        "positive_samples = df[df['template'] == 'positive'].sample(n=100000, random_state=42)\n",
        "negative_samples = df[df['template'] == 'negative'].sample(n=100000, random_state=42)\n",
        "\n",
        "merged_df = pd.concat([neutral_samples, positive_samples, negative_samples], ignore_index=True)\n",
        "merged_df = merged_df.sample(frac=1).reset_index(drop=True)\n",
        "merged_df.to_csv(f'{DATASET_PATH}/ko_sentence_template.csv', encoding='utf-8', index=False)\n",
        "\n",
        "# template code를 int로 변환\n",
        "merged_df.loc[(merged_df['template'] == \"neutral\"), 'template'] = 1   # neutral : 1\n",
        "merged_df.loc[(merged_df['template'] == \"positive\"), 'template'] = 2     # positive : 2\n",
        "merged_df.loc[(merged_df['template'] == \"negative\"), 'template'] = 0    # negative : 0\n",
        "\n",
        "train_dataset, test_dataset = train_test_split(merged_df, test_size=0.25, random_state=11, stratify=merged_df['template'])\n",
        "\n",
        "# 학습과 테스트를 위해 cleaned data를 저장\n",
        "train_dataset.to_csv(f'{MODEL_PATH}/data/cleaned_train.csv', encoding='utf-8')\n",
        "test_dataset.to_csv(f'{MODEL_PATH}/data/cleaned_test.csv', encoding='utf-8')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BshUdpfsj_Ba"
      },
      "source": [
        "### 추출한 최종 데이터 분포 확인"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZlUDKXyZkBlT"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "\n",
        "df = pd.read_csv(f'{DATASET_PATH}/ko_sentence_template.csv')\n",
        "# ==========================\n",
        "# 추출한 data 분포\n",
        "# --------------------------\n",
        "# template\n",
        "# positive    0.333333\n",
        "# neutral     0.333333\n",
        "# negative    0.333333\n",
        "# Name: proportion, dtype: float64\n",
        "print('==========================')\n",
        "print('추출한 data 분포')\n",
        "print('--------------------------')\n",
        "print(df['template'].value_counts(normalize=True))\n",
        "# df['template'].hist(figsize=(5,5))\n",
        "\n",
        "df['template'].hist(bins=20, alpha=0.5, edgecolor = 'black')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4dTrLwiY2YE1"
      },
      "outputs": [],
      "source": [
        "import math, torch, wandb, sys, os\n",
        "\n",
        "py_file_location = f'{MODEL_PATH}'\n",
        "sys.path.append(os.path.abspath(py_file_location))\n",
        "\n",
        "from datetime import datetime, timezone, timedelta\n",
        "import pandas as pd\n",
        "\n",
        "from transformers import TrainingArguments, AutoModelForSequenceClassification, DebertaV2ForSequenceClassification\n",
        "\n",
        "from modules.trainer import CustomTrainer\n",
        "from modules.dataset import CustomDataset\n",
        "from modules.optimizer import get_optimizer\n",
        "from modules.metrics import compute_metrics\n",
        "from modules.utils import load_yaml\n",
        "from modules.split import split"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dU_hY1E4286h"
      },
      "source": [
        "### 메모리 초기화"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "yeenmChplVod"
      },
      "outputs": [],
      "source": [
        "import torch, gc\n",
        "gc.collect()\n",
        "torch.cuda.empty_cache()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nvFV126R7vNL"
      },
      "source": [
        "## 학습"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lAtizVpvzqcY"
      },
      "outputs": [],
      "source": [
        "# Load config\n",
        "config_path = os.path.join(MODEL_PATH, 'config', 'train_config.yaml')\n",
        "config = load_yaml(config_path)\n",
        "\n",
        "# Train Serial\n",
        "kst = timezone(timedelta(hours=9))\n",
        "train_serial = datetime.now(tz=kst).strftime(\"%Y%m%d_%H%M%S\")\n",
        "\n",
        "# Recorder directory\n",
        "OUTPUT_DIR = os.path.join(MODEL_PATH, 'results')\n",
        "os.makedirs(OUTPUT_DIR, exist_ok=True)\n",
        "\n",
        "# Data directory\n",
        "DATA_DIR = os.path.join(MODEL_PATH, 'data', config.DIRECTORY.dataset)\n",
        "\n",
        "#DEVICE\n",
        "# device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "if torch.cuda.is_available() : device = torch.device('cuda')\n",
        "elif torch.backends.mps.is_available() : device = torch.device('mps')\n",
        "else : device=torch.device('cpu')\n",
        "print(f'Using {device}')\n",
        "\n",
        "\n",
        "if __name__ == '__main__':\n",
        "\n",
        "    wandb.login()\n",
        "\n",
        "    run = wandb.init(project='post-sentiment-cls',\n",
        "            config = {\n",
        "                \"learning_rate\": config.TRAIN.learning_rate.pretrained,\n",
        "                \"architecture\": config.MODEL.model_name,\n",
        "                \"dataset\": DATA_DIR + '_train.csv',\n",
        "                \"epochs\": config.TRAIN.num_of_epochs,\n",
        "                \"batch_size\": config.TRAIN.batch_size,\n",
        "                \"num_of_classes\": config.MODEL.num_of_classes\n",
        "                }\n",
        "            )\n",
        "\n",
        "    output_dir      = OUTPUT_DIR\n",
        "    pretrained_link = config.MODEL.pretrained_link[config.MODEL.model_name]\n",
        "    num_of_classes  = config.MODEL.num_of_classes\n",
        "    batch_size      = config.TRAIN.batch_size\n",
        "    num_of_epochs   = config.TRAIN.num_of_epochs\n",
        "    learning_rate   = config.TRAIN.learning_rate.pretrained\n",
        "    max_grad_norm   = config.TRAIN.max_grad_norm\n",
        "    warmup_ratio    = config.TRAIN.warmup_ratio\n",
        "    run_name        = config.MODEL.model_name + train_serial\n",
        "    max_seq_len     = config.TRAIN.max_seq_len\n",
        "\n",
        "    data = pd.read_csv(DATA_DIR + '_train.csv', index_col = 0)\n",
        "\n",
        "    train_df, valid_df = split(data)\n",
        "\n",
        "    train_len = len(train_df)\n",
        "    loader_len = math.ceil(train_len / batch_size)\n",
        "    t_total = loader_len * num_of_epochs\n",
        "    warmup_step = int(t_total * warmup_ratio)\n",
        "\n",
        "    training_args = TrainingArguments(output_dir = output_dir,\n",
        "                                      per_device_train_batch_size = batch_size,\n",
        "                                      per_device_eval_batch_size = batch_size,\n",
        "                                      num_train_epochs = num_of_epochs,\n",
        "                                      learning_rate = learning_rate,\n",
        "                                      max_grad_norm = max_grad_norm,\n",
        "                                      lr_scheduler_type = 'cosine',\n",
        "                                      warmup_ratio = warmup_ratio,\n",
        "                                      evaluation_strategy = 'steps',\n",
        "                                      report_to = 'wandb',\n",
        "                                      run_name = run_name,\n",
        "                                      dataloader_pin_memory = False,\n",
        "                                      logging_steps = 20,\n",
        "                                      load_best_model_at_end = True,\n",
        "                                      save_steps = 500,\n",
        "                                      no_cuda=True\n",
        "                                      )\n",
        "\n",
        "    if config.MODEL.model_name == 'DeBERTa':\n",
        "        model = DebertaV2ForSequenceClassification.from_pretrained(pretrained_link, num_labels=num_of_classes).to(device)\n",
        "    else:\n",
        "        model = AutoModelForSequenceClassification.from_pretrained(pretrained_link, num_labels=num_of_classes).to(device)\n",
        "\n",
        "    optimizer = get_optimizer('AdamW', model, learning_rate, warmup_step, t_total)\n",
        "\n",
        "    train_dataset = CustomDataset(train_df, pretrained_link, max_seq_len)\n",
        "    valid_dataset = CustomDataset(valid_df, pretrained_link, max_seq_len)\n",
        "\n",
        "    trainer = CustomTrainer(model = model,\n",
        "                            args = training_args,\n",
        "                            optimizers = optimizer,\n",
        "                            train_dataset = train_dataset,\n",
        "                            eval_dataset = valid_dataset,\n",
        "                            compute_metrics = compute_metrics\n",
        "                            )\n",
        "\n",
        "    # Train\n",
        "    trainer.train()\n",
        "    trainer.save_model(os.path.join(OUTPUT_DIR, f'{config.MODEL.model_name}_{config.TRAIN.max_seq_len}_{config.TRAIN.loss}_{config.DIRECTORY.dataset}'))\n",
        "    run.finish()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TRctvOdktpTr"
      },
      "source": [
        "## 테스트"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vTTQckEN8AA7"
      },
      "outputs": [],
      "source": [
        "import os, torch, sys, wandb\n",
        "py_file_location = f'{MODEL_PATH}'\n",
        "sys.path.append(os.path.abspath(py_file_location))\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from tqdm import tqdm\n",
        "\n",
        "from transformers import TrainingArguments, AutoModelForSequenceClassification, AutoTokenizer, DebertaV2ForSequenceClassification\n",
        "\n",
        "from modules.metrics import compute_metrics\n",
        "from modules.trainer import CustomTrainer\n",
        "from modules.utils import load_yaml\n",
        "from modules.preprocess import preprocess_infer\n",
        "from modules.dataset import CustomDataset\n",
        "\n",
        "# Load config\n",
        "config_path = os.path.join(MODEL_PATH, 'config', 'inference_config.yaml')\n",
        "config = load_yaml(config_path)\n",
        "\n",
        "# Recorder directory\n",
        "CHECKPOINT_DIR  = os.path.join(MODEL_PATH, 'results', config.TEST.checkpoint_path)\n",
        "OUTPUT_DIR = os.path.join(CHECKPOINT_DIR, 'test_results')\n",
        "os.makedirs(OUTPUT_DIR, exist_ok=True)\n",
        "\n",
        "# Data directory\n",
        "DATA_DIR = os.path.join(MODEL_PATH, 'data', config.TEST.directory.dataset)\n",
        "\n",
        "#DEVICE\n",
        "# device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "if torch.cuda.is_available() : device = torch.device('cuda')\n",
        "elif torch.backends.mps.is_available() : device = torch.device('mps')\n",
        "else : device=torch.device('cpu')\n",
        "print(f'Using {device}')\n",
        "\n",
        "# wandb.init(project='text-binary-cls',\n",
        "#            config = {\n",
        "#               \"architecture\": config.MODEL.model_name,\n",
        "#               \"max_seq_len\": config.MODEL.max_seq_len,\n",
        "#               \"num_of_classes\": config.MODEL.num_of_classes\n",
        "#             }\n",
        "#            )\n",
        "\n",
        "def test():\n",
        "\n",
        "    output_dir      = OUTPUT_DIR\n",
        "    pretrained_link = config.MODEL.pretrained_link[config.MODEL.model_name]\n",
        "    num_of_classes  = config.MODEL.num_of_classes\n",
        "    max_seq_len     = config.MODEL.max_seq_len\n",
        "    checkpoint_path = CHECKPOINT_DIR\n",
        "\n",
        "    test_args = TrainingArguments(output_dir = output_dir,\n",
        "                                      per_device_eval_batch_size = config.TEST.batch_size,\n",
        "                                      report_to = 'wandb',\n",
        "                                      dataloader_pin_memory = False,\n",
        "                                      do_eval = True,\n",
        "                                      no_cuda=True\n",
        "                                      )\n",
        "\n",
        "    if config.MODEL.model_name == 'DeBERTa':\n",
        "        model = DebertaV2ForSequenceClassification.from_pretrained(checkpoint_path, num_labels=num_of_classes).to(device)\n",
        "    else:\n",
        "        model = AutoModelForSequenceClassification.from_pretrained(checkpoint_path, num_labels=num_of_classes).to(device)\n",
        "\n",
        "\n",
        "    data = pd.read_csv(DATA_DIR + '_test.csv', index_col = 0)\n",
        "    data = preprocess_infer(data)\n",
        "    dataset = CustomDataset(data, pretrained_link, max_seq_len)\n",
        "\n",
        "    # wandb.login()\n",
        "    trainer = CustomTrainer(model = model,\n",
        "                            args = test_args,\n",
        "                            compute_metrics = compute_metrics,\n",
        "                            eval_dataset = dataset\n",
        "                            )\n",
        "    trainer.evaluate()\n",
        "\n",
        "def inference(data):\n",
        "\n",
        "    output_dir      = OUTPUT_DIR\n",
        "    pretrained_link = config.MODEL.pretrained_link[config.MODEL.model_name]\n",
        "    num_of_classes  = config.MODEL.num_of_classes\n",
        "    max_seq_len     = config.MODEL.max_seq_len\n",
        "    checkpoint_path = CHECKPOINT_DIR\n",
        "\n",
        "    print('=' * 50)\n",
        "    print('Get Model & Tokenizer')\n",
        "    print('=' * 50)\n",
        "\n",
        "    test_args = TrainingArguments(output_dir=output_dir,\n",
        "                                    dataloader_pin_memory = False,\n",
        "                                    do_predict = True,\n",
        "                                    no_cuda=True\n",
        "                                    )\n",
        "\n",
        "    if config.MODEL.model_name == 'DeBERTa':\n",
        "        model = DebertaV2ForSequenceClassification.from_pretrained(checkpoint_path, num_labels=num_of_classes).to(device)\n",
        "    else:\n",
        "        model = AutoModelForSequenceClassification.from_pretrained(checkpoint_path, num_labels=num_of_classes).to(device)\n",
        "\n",
        "    trainer = CustomTrainer(model = model,\n",
        "                            args = test_args,\n",
        "                            compute_metrics = compute_metrics,\n",
        "                            )\n",
        "\n",
        "    tokenizer = AutoTokenizer.from_pretrained(pretrained_link)\n",
        "\n",
        "    print('=' * 50)\n",
        "    print('Tokenizing...')\n",
        "    print('=' * 50)\n",
        "\n",
        "    items = list()\n",
        "\n",
        "    if type(data) == str:\n",
        "        item = {key: torch.tensor(val).to(device) for key, val in tokenizer(data,\n",
        "                                                                            truncation = True,\n",
        "                                                                            padding = 'max_length',\n",
        "                                                                            max_length = max_seq_len).items()}\n",
        "        items.append(item)\n",
        "\n",
        "    elif type(data) == pd.DataFrame:\n",
        "        for name in tqdm(data['sentence_r']):\n",
        "            item = {key: torch.tensor(val).to(device) for key, val in tokenizer(name,\n",
        "                                                                                truncation = True,\n",
        "                                                                                padding = 'max_length',\n",
        "                                                                                max_length = max_seq_len).items()}\n",
        "            items.append(item)\n",
        "\n",
        "    print('=' * 50)\n",
        "    print('Predicting...')\n",
        "    print('=' * 50)\n",
        "\n",
        "    test_results = trainer.predict(items)\n",
        "    label_ids = np.argmax(test_results[0], axis = 1)\n",
        "\n",
        "    if type(data) == str:\n",
        "        return config.LABELING[label_ids[0]]\n",
        "\n",
        "    elif type(data) == pd.DataFrame:\n",
        "        data['template_r'] = label_ids\n",
        "        # data['template_r'] = data['template_r'].replace(config.LABELING.keys(), config.LABELING.values())\n",
        "        return data\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    print('=' * 50)\n",
        "    print('Preprocessing...')\n",
        "    print('=' * 50)\n",
        "\n",
        "# 실제 label과 추론 label 비교\n",
        "    # A = inference(preprocess_infer(pd.read_csv(f'{MODEL_PATH}/data/cleaned_test.csv', index_col = 0)))\n",
        "    # A.to_csv(f'{MODEL_PATH}/results/{config.MODEL.model_name}/inferenced.csv')\n",
        "    # print(A)\n",
        "\n",
        "# 문장 입력\n",
        "    B = inference(preprocess_infer('텀블벅 프로젝트에 함께해 주셔서 고맙습니다. '))\n",
        "    print(B)\n",
        "\n",
        "# 테스트 데이터셋 활용\n",
        "    # a = test()\n",
        "    # print(a)\n",
        "    # wandb.finish()\n",
        "    # pass\n"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [
        "AZHobXSbtpTp"
      ],
      "gpuType": "A100",
      "machine_shape": "hm",
      "provenance": []
    },
    "kernelspec": {
      "display_name": ".venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.11"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
